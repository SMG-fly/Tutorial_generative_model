# configs/config.yaml

# 1. 일반 설정
general:
  name: Discrete_Diffusion_no_Guidance # 실험 이름
  seed: 42
  debug: false
  test_only: false # True로 설정하면 학습 없이 테스트만 수행
  resume: null # 이전 checkpoint 경로
  save_chain_steps: false # chain_steps(중간 단계)들도 저장할지 여부
  gpus: 4
  check_val_every_n_epochs: 10
  wandb: online # 'online', 'offline', 'disabled' # 일단은 비활성화

  samples_to_generate: 100
  samples_to_save: 100
  chains_to_save: 20

  final_model_samples_to_generate: 100
  final_model_samples_to_save: 100
  final_model_chains_to_save: 20


# 2. 데이터 설정
dataset:
  name: smiles # 이제 이거 안 쓰는듯 # To do: check
  datadir: Dataset/split_wo_logp/
  tokenizer_path: Dataset/tokenizer_w_special_tokens.pkl
  max_length: 128

# 3. 모델 설정
model: # 모델 설정 전체적으로 괜찮은지 확인할 것 # 그냥 최대한 간단한 구조 하는 게 나을 수도?
  embedding_dim: 256 # 임베딩 차원 # x0
  num_heads: 8 # 헤드 수 # Transformer에서 멀티헤드 어텐션의 헤드 수
  num_layers: 6 # x0 model 레이어 수 
  extra_features: null  # or logp
  y_hidden_dim: 64 # y의 hidden dim # y는 logp로 설정
  lambda_train: 0 # y의 loss는 일단 무시
  noise_schedule: 'absorbing'
  diffusion_steps: 10 # diffution step 수(T) 
  transition: 'absorbing'
  abs_state: 3 # mask token index

# 4. 학습 설정
train:
  batch_size: 64
  lr: 1e-4
  weight_decay: 1e-6
  n_epochs: 1000
  clip_grad: 1.0 # gradient clipping # gradient가 너무 커지는 것을 방지하기 위한 설정
  save_model: true # 모델 저장 여부

# 5. Trainer 설정 (Lightning)
trainer:
  log_every_steps: 50 # 로그 출력 간격 
  number_chain_steps: 10 # chain step 수 
  early_stopping: 10 # code에 있는 early stopping이 내가 생각하는 것과 다른 거 같아. # To do: check

# 6. Save 설정
save:
  # save_dir: SmD3PM/Results/
  checkpoint_path: Results/checkpoints/
  generated_sequence_path: Results/generated_sequences_results/  
  chain_step_result_path: Results/sequence_chain_results/
  

